# Data-Engineering-Projects
Design data models, build data warehouses and data lakes, automate data pipelines, and work with massive datasets




**Data Modeling with Postgres
| Model user activity data to create a database and ETL pipeline in Postgres for a music steaming app
[Project 1b - *Data Modeling with Cassandra*](https://github.com/cathydo178/Data-Engineering-Projects/tree/master/Project%201%20-%20Data%20Modeling/Project%201b%20-%20Data%20Modeling%20with%20Cassandra) | 
[Project 2 - *Data Warehouse*](https://github.com/cathydo178/Data-Engineering-Projects/tree/master/Project%202%20-%20Data%20Warehouse) | Build an ETL piepline that extracts data from S3, stages them in Redshift, and transform data into a set of dimensional tables
[Project 3 - *Data Lake*](https://github.com/cathydo178/Data-Engineering-Projects/tree/master/Project%203%20-%20Data%20Lake) | Build a data lake and an ETL pipeline in Sparks that loads data from S3, processes the data into analytic tables, and loads them back into S3
[Project 4 - *Data Pipelines*](https://github.com/cathydo178/Data-Engineering-Projects/tree/master/Project%204%20-%20Data%20pipeline) | Creating and automating a set of data pipelines with Apache Airflow, monitoring and debugging production pipelines
[Project 5 - *Personal Project*](https://github.com/cathydo178/Data-Engineering-Projects/tree/master/Project%205%20-%20Personal%20Project) | Work with more than 1 million rows of dataset, and create a data pipeline using Apache Spark


# Distributed-Systems 

### Data Modeling with Postgres

- Model user activity data to create a database and ETL pipeline in Postgres for a music steaming app

### Data Modeling with Cassandra

- Model event data to create a non-relational database and ETL pipeline for a music streaming app

### Data Warehouse

- Build an ETL piepline that extracts data from S3, stages them in Redshift, and transform data into a set of dimensional tables

### Data Lake

- Build a data lake and an ETL pipeline in Sparks that loads data from S3, processes the data into analytic tables, and loads them back into S3

### Data Lake

- Build a data lake and an ETL pipeline in Sparks that loads data from S3, processes the data into analytic tables, and loads them back into S3

### Simple Amazon Dynamo

- Developed an Amazon dynamo style distributed key-value storage system
- Implemented data replication
- Implemented data partitioning
- Handled node failures while continuing to provide availability and linearizability